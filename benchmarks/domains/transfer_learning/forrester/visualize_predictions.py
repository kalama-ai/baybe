#!/usr/bin/env python3
"""Visualize transfer learning model predictions from stored JSON data.

This script reads prediction JSON files generated by store_tl_predictions.py
and creates comprehensive visualizations showing how model predictions evolve
throughout DOE iterations.

Usage:
    python visualize_predictions.py  # Visualize all JSON files in current dir
    python visualize_predictions.py --input_dir results/ --output_dir plots/
    python visualize_predictions.py --benchmark forrester_noise_05  # Specific benchmark
"""

import argparse
import json
import os
import sys
import glob
from pathlib import Path
from typing import Any, Dict, List

# Add BayBE root directory to Python path
script_dir = Path(__file__).resolve().parent
baybe_root = script_dir.parent.parent.parent.parent
sys.path.insert(0, str(baybe_root))

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from tqdm import tqdm

from benchmarks.domains.transfer_learning.forrester.base import (
    TARGET_CONFIG,
    forrester_function,
)

# Set plotting style
plt.style.use("default")
sns.set_palette("colorblind")

# Color scheme
COLORS = {
    "source_function": "gray",
    "target_function": "black", 
    "prediction_mean": "green",
    "prediction_band": "lightgreen",
    "source_data": "gold",
    "target_data": "red",
}

MODEL_ORDER = ["index_kernel", "source_prior", "mhgp", "shgp", "0_reduced", "0_full"]
MODEL_LABELS = {
    "index_kernel": "Index Kernel GP",
    "source_prior": "Source Prior GP", 
    "mhgp": "MHGP",
    "shgp": "SHGP",
    "0_reduced": "GP (Target Only)",
    "0_full": "GP (No Source)"
}


def load_predictions_data(json_file: str) -> Dict[str, Any]:
    """Load predictions data from JSON file.
    
    Args:
        json_file: Path to JSON file
        
    Returns:
        Loaded predictions data
    """
    with open(json_file, 'r') as f:
        return json.load(f)


def generate_reference_functions(x_grid: np.ndarray, target_config: Dict, source_config: Dict) -> Dict[str, np.ndarray]:
    """Generate reference source and target functions.
    
    Args:
        x_grid: X values for evaluation
        target_config: Target function configuration
        source_config: Source function configuration
        
    Returns:
        Dictionary with source and target function values
    """
    import torch
    
    x_tensor = torch.tensor(x_grid, dtype=torch.float32)
    
    target_vals = forrester_function(x_tensor, **target_config).numpy().flatten()
    source_vals = forrester_function(x_tensor, **source_config).numpy().flatten()
    
    return {
        "target": target_vals,
        "source": source_vals
    }


def plot_predictions_on_axis(ax, x_grid: np.ndarray, predictions: Dict, 
                           reference_funcs: Dict, source_data: Dict = None,
                           observed_data: List = None, show_source_pred: Dict = None,
                           title: str = "", show_legend: bool = False):
    """Plot predictions and reference data on a single axis.
    
    Args:
        ax: Matplotlib axis
        x_grid: X values for predictions
        predictions: Prediction data with mean and std
        reference_funcs: Reference source and target functions
        source_data: Source data points (optional)
        observed_data: Observed target data points (optional)
        show_source_pred: Source model predictions (optional)
        title: Plot title
        show_legend: Whether to show legend
    """
    # Plot reference functions
    ax.plot(x_grid, reference_funcs["source"], "--", color=COLORS["source_function"], 
           linewidth=1.5, alpha=0.7, label="Source function")
    ax.plot(x_grid, reference_funcs["target"], "-", color=COLORS["target_function"], 
           linewidth=2, label="Target function")
    
    # Plot source model predictions (if available)
    if show_source_pred is not None:
        source_mean = np.array(show_source_pred["mean"])
        source_std = np.array(show_source_pred["std"])
        ax.plot(x_grid, source_mean, "--", color="lightcoral", linewidth=1.5, 
               alpha=0.6, label="Source model")
        ax.fill_between(x_grid, source_mean - source_std, source_mean + source_std, 
                       alpha=0.2, color="lightcoral")
    
    # Plot model predictions
    if predictions and "mean" in predictions:
        pred_mean = np.array(predictions["mean"])
        pred_std = np.array(predictions["std"])
        
        ax.plot(x_grid, pred_mean, "-", color=COLORS["prediction_mean"], 
               linewidth=2, label="Model prediction")
        ax.fill_between(x_grid, pred_mean - pred_std, pred_mean + pred_std, 
                       alpha=0.3, color=COLORS["prediction_band"], 
                       label="Â±1Ïƒ")
    
    # Plot data points
    if source_data:
        ax.scatter(source_data["x"], source_data["Target"], 
                  c=COLORS["source_data"], s=30, alpha=0.8, 
                  label="Source data", zorder=5)
    
    if observed_data:
        obs_x = [point["x"] for point in observed_data]
        obs_y = [point["Target"] for point in observed_data]
        ax.scatter(obs_x, obs_y, c=COLORS["target_data"], s=60, 
                  marker="s", alpha=0.9, label="Target data", zorder=6)
    
    # Formatting
    ax.set_xlim(0, 1)
    ax.grid(True, alpha=0.3)
    ax.set_title(title, fontsize=10)
    
    if show_legend:
        ax.legend(fontsize=8, loc="best")


def create_mc_iteration_plot(data: Dict[str, Any], mc_iter: int, output_dir: str):
    """Create plot for specific MC iteration (rows=models, cols=DOE+1).
    
    Args:
        data: Complete predictions data
        mc_iter: MC iteration index
        output_dir: Output directory for plots
    """
    benchmark_name = data["benchmark_name"]
    n_doe_iterations = data["n_doe_iterations"]
    x_grid = np.array(data["prediction_grid"])
    
    # Generate reference functions
    reference_funcs = generate_reference_functions(
        x_grid, data["target_config"], data["source_config"]
    )
    
    mc_data = data["mc_iterations"][mc_iter]
    source_data = mc_data["source_data"]
    
    # Setup plot
    n_models = len(MODEL_ORDER)
    n_cols = n_doe_iterations + 1  # +1 for initial source predictions
    
    fig, axes = plt.subplots(n_models, n_cols, figsize=(3 * n_cols, 2.5 * n_models),
                           sharex=True, sharey="row")
    
    if n_models == 1:
        axes = axes.reshape(1, -1)
    if n_cols == 1:
        axes = axes.reshape(-1, 1)
    
    # Calculate global y-limits per row for consistent scaling
    y_limits = []
    for model_idx, model_name in enumerate(MODEL_ORDER):
        if model_name not in mc_data["models"]:
            y_limits.append((-2, 8))  # Default limits
            continue
            
        model_data = mc_data["models"][model_name]
        all_y_vals = []
        
        # Collect y values from all predictions and reference functions
        all_y_vals.extend(reference_funcs["target"])
        all_y_vals.extend(reference_funcs["source"])
        
        # Add prediction values
        for doe_data in model_data["doe_iterations"]:
            if doe_data["predictions"]["mean"]:
                pred_mean = np.array(doe_data["predictions"]["mean"])
                pred_std = np.array(doe_data["predictions"]["std"])
                all_y_vals.extend(pred_mean + pred_std)
                all_y_vals.extend(pred_mean - pred_std)
        
        # Calculate limits with padding
        y_min, y_max = np.min(all_y_vals), np.max(all_y_vals)
        y_range = y_max - y_min
        padding = 0.1 * y_range if y_range > 0 else 1.0
        y_limits.append((y_min - padding, y_max + padding))
    
    # Create plots
    for model_idx, model_name in enumerate(MODEL_ORDER):
        if model_name not in mc_data["models"]:
            # Model not available, fill with empty plots
            for col in range(n_cols):
                ax = axes[model_idx, col]
                ax.text(0.5, 0.5, f"{MODEL_LABELS.get(model_name, model_name)}\nNot Available", 
                       ha="center", va="center", transform=ax.transAxes)
                ax.set_ylim(y_limits[model_idx])
            continue
            
        model_data = mc_data["models"][model_name]
        
        for col in range(n_cols):
            ax = axes[model_idx, col]
            
            if col == 0:
                # Initial column: source predictions (iteration 0)
                show_source_pred = model_data.get("source_predictions")
                observed_data = []
                title = f"Initial\n({MODEL_LABELS.get(model_name, model_name)})"
                
                # For initial column, show source predictions as main predictions
                if show_source_pred:
                    predictions = show_source_pred
                    show_source_pred = None  # Don't show twice
                else:
                    predictions = {"mean": [0] * len(x_grid), "std": [0] * len(x_grid)}
                    
            else:
                # DOE iteration predictions
                doe_idx = col - 1
                if doe_idx < len(model_data["doe_iterations"]):
                    doe_data = model_data["doe_iterations"][doe_idx]
                    predictions = doe_data["predictions"]
                    observed_data = doe_data["observed_data_so_far"]
                    title = f"DOE {doe_idx + 1}"
                    show_source_pred = None  # Don't show source predictions after initial
                else:
                    predictions = {"mean": [0] * len(x_grid), "std": [0] * len(x_grid)}
                    observed_data = []
                    title = f"DOE {doe_idx + 1}"
                    show_source_pred = None
            
            # Plot on axis
            show_legend = (model_idx == 0 and col == 0)  # Legend only on first plot
            plot_predictions_on_axis(
                ax=ax, x_grid=x_grid, predictions=predictions,
                reference_funcs=reference_funcs, source_data=source_data,
                observed_data=observed_data, show_source_pred=show_source_pred,
                title=title, show_legend=show_legend
            )
            
            # Set y-limits for consistent scaling within row
            ax.set_ylim(y_limits[model_idx])
            
            # Labels
            if model_idx == n_models - 1:
                ax.set_xlabel("x")
            if col == 0:
                ax.set_ylabel(f"{MODEL_LABELS.get(model_name, model_name)}")
    
    plt.suptitle(f"{benchmark_name} - MC Iteration {mc_iter}", fontsize=14, y=0.95)
    plt.tight_layout(rect=[0, 0.03, 1, 0.93])
    
    # Save plot
    os.makedirs(f"{output_dir}/mc_iterations", exist_ok=True)
    output_file = f"{output_dir}/mc_iterations/{benchmark_name}_mc_iteration_{mc_iter}.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    plt.close()


def create_model_plot(data: Dict[str, Any], model_name: str, output_dir: str):
    """Create plot for specific model (rows=MC iterations, cols=DOE+1).
    
    Args:
        data: Complete predictions data
        model_name: Name of the model
        output_dir: Output directory for plots
    """
    if model_name not in MODEL_ORDER:
        return
        
    benchmark_name = data["benchmark_name"]
    n_mc_iterations = data["n_mc_iterations"]
    n_doe_iterations = data["n_doe_iterations"]
    x_grid = np.array(data["prediction_grid"])
    
    # Generate reference functions
    reference_funcs = generate_reference_functions(
        x_grid, data["target_config"], data["source_config"]
    )
    
    # Setup plot
    n_cols = n_doe_iterations + 1  # +1 for initial source predictions
    
    fig, axes = plt.subplots(n_mc_iterations, n_cols, 
                           figsize=(3 * n_cols, 2.5 * n_mc_iterations),
                           sharex=True, sharey=True)
    
    if n_mc_iterations == 1:
        axes = axes.reshape(1, -1)
    if n_cols == 1:
        axes = axes.reshape(-1, 1)
    
    # Calculate global y-limits for consistent scaling
    all_y_vals = []
    all_y_vals.extend(reference_funcs["target"])
    all_y_vals.extend(reference_funcs["source"])
    
    for mc_data in data["mc_iterations"]:
        if model_name in mc_data["models"]:
            model_data = mc_data["models"][model_name]
            for doe_data in model_data["doe_iterations"]:
                if doe_data["predictions"]["mean"]:
                    pred_mean = np.array(doe_data["predictions"]["mean"])
                    pred_std = np.array(doe_data["predictions"]["std"])
                    all_y_vals.extend(pred_mean + pred_std)
                    all_y_vals.extend(pred_mean - pred_std)
    
    y_min, y_max = np.min(all_y_vals), np.max(all_y_vals)
    y_range = y_max - y_min
    padding = 0.1 * y_range if y_range > 0 else 1.0
    y_limits = (y_min - padding, y_max + padding)
    
    # Create plots
    for mc_idx in range(n_mc_iterations):
        mc_data = data["mc_iterations"][mc_idx]
        source_data = mc_data["source_data"]
        
        if model_name not in mc_data["models"]:
            # Model not available for this MC iteration
            for col in range(n_cols):
                ax = axes[mc_idx, col]
                ax.text(0.5, 0.5, f"{MODEL_LABELS.get(model_name, model_name)}\nNot Available", 
                       ha="center", va="center", transform=ax.transAxes)
            continue
            
        model_data = mc_data["models"][model_name]
        
        for col in range(n_cols):
            ax = axes[mc_idx, col]
            
            if col == 0:
                # Initial column: source predictions
                show_source_pred = model_data.get("source_predictions")
                observed_data = []
                title = "Initial" if mc_idx == 0 else ""
                
                # Show source predictions as main predictions
                if show_source_pred:
                    predictions = show_source_pred
                    show_source_pred = None
                else:
                    predictions = {"mean": [0] * len(x_grid), "std": [0] * len(x_grid)}
                    
            else:
                # DOE iteration predictions
                doe_idx = col - 1
                if doe_idx < len(model_data["doe_iterations"]):
                    doe_data = model_data["doe_iterations"][doe_idx]
                    predictions = doe_data["predictions"]
                    observed_data = doe_data["observed_data_so_far"]
                    title = f"DOE {doe_idx + 1}" if mc_idx == 0 else ""
                    show_source_pred = None
                else:
                    predictions = {"mean": [0] * len(x_grid), "std": [0] * len(x_grid)}
                    observed_data = []
                    title = f"DOE {doe_idx + 1}" if mc_idx == 0 else ""
                    show_source_pred = None
            
            # Plot on axis
            show_legend = (mc_idx == 0 and col == 0)  # Legend only on first plot
            plot_predictions_on_axis(
                ax=ax, x_grid=x_grid, predictions=predictions,
                reference_funcs=reference_funcs, source_data=source_data,
                observed_data=observed_data, show_source_pred=show_source_pred,
                title=title, show_legend=show_legend
            )
            
            # Set consistent y-limits
            ax.set_ylim(y_limits)
            
            # Labels
            if mc_idx == n_mc_iterations - 1:
                ax.set_xlabel("x")
            if col == 0:
                ax.set_ylabel(f"MC {mc_idx}")
    
    model_title = MODEL_LABELS.get(model_name, model_name)
    plt.suptitle(f"{benchmark_name} - {model_title}", fontsize=14, y=0.95)
    plt.tight_layout(rect=[0, 0.03, 1, 0.93])
    
    # Save plot
    os.makedirs(f"{output_dir}/models", exist_ok=True)
    output_file = f"{output_dir}/models/{benchmark_name}_{model_name}.png"
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    plt.close()


def visualize_benchmark(json_file: str, output_base_dir: str):
    """Visualize a single benchmark from JSON file.
    
    Args:
        json_file: Path to JSON file with predictions
        output_base_dir: Base output directory for plots
    """
    # Load data
    data = load_predictions_data(json_file)
    benchmark_name = data["benchmark_name"]
    
    print(f"Visualizing {benchmark_name}...")
    
    # Create output directory
    output_dir = os.path.join(output_base_dir, benchmark_name)
    os.makedirs(output_dir, exist_ok=True)
    
    # Create MC iteration plots
    n_mc_iterations = data["n_mc_iterations"]
    print(f"  Creating {n_mc_iterations} MC iteration plots...")
    for mc_iter in tqdm(range(n_mc_iterations), desc="MC plots"):
        create_mc_iteration_plot(data, mc_iter, output_dir)
    
    # Create model plots
    print(f"  Creating {len(MODEL_ORDER)} model plots...")
    for model_name in tqdm(MODEL_ORDER, desc="Model plots"):
        create_model_plot(data, model_name, output_dir)
    
    print(f"  âœ… Completed {benchmark_name}")


def main():
    """Main execution function."""
    # Get script directory for default paths
    script_dir = Path(__file__).resolve().parent
    
    parser = argparse.ArgumentParser(description="Visualize TL model predictions from JSON files")
    parser.add_argument("--input_dir", type=str, default=str(script_dir), 
                       help="Directory containing prediction JSON files")
    parser.add_argument("--output_dir", type=str, default=str(script_dir / "predictions"), 
                       help="Output directory for plots")
    parser.add_argument("--benchmark", type=str, 
                       help="Visualize only specific benchmark")
    
    args = parser.parse_args()
    
    # Find JSON files
    if args.benchmark:
        json_files = [f"{args.input_dir}/{args.benchmark}_predictions.json"]
        # Verify file exists
        if not os.path.exists(json_files[0]):
            print(f"âŒ File not found: {json_files[0]}")
            return
    else:
        pattern = os.path.join(args.input_dir, "*_predictions.json")
        json_files = glob.glob(pattern)
    
    if not json_files:
        print(f"âŒ No prediction JSON files found in {args.input_dir}")
        return
    
    print(f"ðŸ“Š Found {len(json_files)} benchmark files to visualize")
    
    # Create base output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Process each benchmark
    for json_file in json_files:
        try:
            visualize_benchmark(json_file, args.output_dir)
        except Exception as e:
            print(f"âŒ Failed to visualize {json_file}: {e}")
            import traceback
            traceback.print_exc()
    
    print(f"ðŸŽ‰ All visualizations completed! Check {args.output_dir}/")


if __name__ == "__main__":
    main()